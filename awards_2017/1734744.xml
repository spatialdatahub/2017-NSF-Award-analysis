<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NCS-FO: Active Listening and Attention in 3D Natural Scenes</AwardTitle>
<AwardEffectiveDate>08/01/2017</AwardEffectiveDate>
<AwardExpirationDate>07/31/2021</AwardExpirationDate>
<AwardAmount>948067</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>11040000</Code>
<Directorate>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<LongName>Division Of Undergraduate Education</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ellen Carpenter</SignBlockName>
</ProgramOfficer>
<AbstractNarration>As humans and other animals move around, the distance and direction between their bodies and objects in their environment are constantly changing.  Judging the position of objects, and readjusting body movements to steer around the objects, requires a constantly updated map of three-dimensional space in the brain.  Generating this map, and keeping it updated during movement, requires dynamic interaction between visual or auditory cues, attention, and behavioral output.  An understanding of how spatial perception is generated in the brain comes from decades of research using visual or auditory stimuli under restricted conditions.  Far less is known about the dynamics of how natural scenes are represented in freely moving animals.  This project will bridge this gap by studying how freely flying bats navigate through their environment using echolocation.  Specifically, a team of engineers and neuroscientists will investigate how the bat brain processes information associated with flight navigation. The project team will provide education and training in engineering and science to public school, undergraduate and graduate students, and to postdoctoral researchers. This research will also contribute to a rich library of materials, including videos and a website, which will be available to educators and scientists working in both the private and public sectors.&lt;br/&gt;&lt;br/&gt;This project leverages innovative engineering tools, cutting-edge neuroscience methods and neuroethological modeling to pursue a multidisciplinary investigation of dynamic feedback between 3D scene representation, attention and action-selection in freely moving animals engaged in natural tasks. The echolocating bat, the subject of the project's research, actively produces the acoustic signals that it uses to represent natural scenes and therefore provides direct access to the sensory information that guides behavior. The specific goals of the project are to test the hypotheses that 1) natural scene representation operates through the interplay between sensory processing, adaptive motor behaviors, and attentional feedback, 2) spatio-temporal responses to sensory streams across ensembles of neurons sharpen when an animal adapts its behavior to attend to selected targets, and 3) spatio-temporal sharpening of neural responses enables figure-ground segregation in the natural environment. The project integrates 1) novel acoustic measurements and computational analyses to represent the sonar scene based on reconstructions of the bat's sonar transmitter and receiver characteristics, combined with a 3D acoustic model of the environment, 2) quantitative analysis of the echolocating bat's adaptive echolocation and flight behaviors as it negotiates complex environments, 3) multichannel neural telemetry recordings from the midbrain of the free-flying bat as it attends to targets, obstacles and other acoustic signals in its surroundings, and 4) computational modeling of auditory system architecture, attention and working memory mechanisms.  Collectively, this research will deepen the understanding of behavioral modulation of natural scene representation. &lt;br/&gt;&lt;br/&gt;This project is funded by Integrative Strategies for Understanding Neural and Cognitive Systems (NSF-NCS), a multidisciplinary program jointly supported by the Directorates for Computer and Information Science and Engineering (CISE), Education and Human Resources (EHR), Engineering (ENG), and Social, Behavioral, and Economic Sciences (SBE).</AbstractNarration>
<MinAmdLetterDate>08/07/2017</MinAmdLetterDate>
<MaxAmdLetterDate>08/07/2017</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>1734744</AwardID>
<Investigator>
<FirstName>Rajat</FirstName>
<LastName>Mittal</LastName>
<EmailAddress>mittal@jhu.edu</EmailAddress>
<StartDate>08/07/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Mounya</FirstName>
<LastName>Elhilali</LastName>
<EmailAddress>mounya@jhu.edu</EmailAddress>
<StartDate>08/07/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Cynthia</FirstName>
<LastName>Moss</LastName>
<EmailAddress>cynthia.moss@jhu.edu</EmailAddress>
<StartDate>08/07/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Susanne</FirstName>
<LastName>Sterbing-D'Angelo</LastName>
<EmailAddress>ssterbi1@jhu.edu</EmailAddress>
<StartDate>08/07/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Johns Hopkins University</Name>
<CityName>Baltimore</CityName>
<ZipCode>212182686</ZipCode>
<PhoneNumber>4439971898</PhoneNumber>
<StreetAddress>1101 E 33rd St</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
</Institution>
<ProgramElement>
<Code>7980</Code>
<Text>Core R&amp;D Programs</Text>
</ProgramElement>
<ProgramElement>
<Code>8624</Code>
<Text>IntgStrat Undst Neurl&amp;Cogn Sys</Text>
</ProgramElement>
<ProgramReference>
<Code>8089</Code>
<Text>Understanding the Brain/Cognitive Scienc</Text>
</ProgramReference>
<ProgramReference>
<Code>8091</Code>
<Text>BRAIN Initiative Res Support</Text>
</ProgramReference>
<ProgramReference>
<Code>8551</Code>
<Text>IntgStrat Undst Neurl&amp;Cogn Sys</Text>
</ProgramReference>
<ProgramReference>
<Code>8817</Code>
<Text>STEM Learning &amp; Learning Environments</Text>
</ProgramReference>
</Award>
</rootTag>
