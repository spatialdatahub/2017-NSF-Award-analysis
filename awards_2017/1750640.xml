<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: A Stable Foundation for Trustworthy Data Analysis</AwardTitle>
<AwardEffectiveDate>02/01/2018</AwardEffectiveDate>
<AwardExpirationDate>01/31/2023</AwardExpirationDate>
<AwardAmount>102716</AwardAmount>
<AwardInstrument>
<Value>Continuing grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tracy J. Kimbrel</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Every day, massive amounts of data are collected, analyzed, and used to make high-stakes decisions, raising many questions about how to use this data in a trustworthy manner.  This project is about two such questions: (1) How can researchers prevent false discovery, and use data to learn meaningful facts about a population without overfitting to that data?  Despite decades of research into methods for preventing false discovery, it remains a vexing problem for the scientific community.  (2) How can researchers use valuable but sensitive data to learn about a population without compromising the privacy of individuals in that data?  This task has proven to be quite delicate, and there have been several high profile attacks on supposedly anonymous datasets, causing a lack of confidence in the most commonly used approaches.  &lt;br/&gt;&lt;br/&gt;Although they may seem unrelated, surprisingly, both of these questions can be addressed using stable algorithms---algorithms that are insensitive to small changes in their inputs.  In the past decade, differential privacy emerged as a strong form of algorithmic stability that guarantees a high degree of individual privacy, yet admits highly accurate data analysis.  More recently, differential privacy has been shown to prevent false discovery in interactive data analysis---the common scenario where the same dataset is analyzed repeatedly, which has been implicated in a "statistical crisis in science."&lt;br/&gt;&lt;br/&gt;This project will take a unified approach to advancing the state-of-the-art in privacy and false discovery via algorithmic stability.  The main outcomes of this project will be building the theoretical foundations of interactive data analysis, developing new computationally efficient stable algorithms for central problems in these areas, understanding the limits of privacy and interactive data analysis both in theory and in practice, and broadening the reach of algorithmic stability to address other challenges in trustworthy data analysis.</AbstractNarration>
<MinAmdLetterDate>01/08/2018</MinAmdLetterDate>
<MaxAmdLetterDate>01/08/2018</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>1750640</AwardID>
<Investigator>
<FirstName>Jonathan</FirstName>
<LastName>Ullman</LastName>
<EmailAddress>jullman@ccs.neu.edu</EmailAddress>
<StartDate>01/08/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Northeastern University</Name>
<CityName>BOSTON</CityName>
<ZipCode>021155005</ZipCode>
<PhoneNumber>6173733004</PhoneNumber>
<StreetAddress>360 HUNTINGTON AVE</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
</Institution>
<ProgramElement>
<Code>7796</Code>
<Text>ALGORITHMIC FOUNDATIONS</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER: FACULTY EARLY CAR DEV</Text>
</ProgramReference>
<ProgramReference>
<Code>7926</Code>
<Text>ALGORITHMS</Text>
</ProgramReference>
</Award>
</rootTag>
