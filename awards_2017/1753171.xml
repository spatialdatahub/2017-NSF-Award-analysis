<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Calibrating Regularization for Enhanced Statistical Inference</AwardTitle>
<AwardEffectiveDate>07/01/2018</AwardEffectiveDate>
<AwardExpirationDate>06/30/2023</AwardExpirationDate>
<AwardAmount>79994</AwardAmount>
<AwardInstrument>
<Value>Continuing grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Gabor J. Szekely</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Volumes of data that seemed unimaginable only a decade ago are now ubiquitous in scientific research. Investment decisions are based on prices, updated every microsecond, for thousands of securities; atmospheric scientists use multiresolution satellite images to understand climate change; and internet companies exploit massive music collections to infer trends in tastes and preferences. Rigorous analysis of these large datasets requires a balance between computational constraints and statistical performance, and operationalizing such tradeoffs involves a combination of algorithmic and design-based approximations. For example, decreasing the resolution of an image or subsampling high-throughput data enables faster computations but removes potentially valuable information. At the same time, elaborate explanations of the scientific process are credible because they account for real-world complexity, but estimating complex models requires both more data and larger computers. The proposed work investigates the viability of modern statistical and machine learning methodologies for answering applied scientific questions. The project will create new algorithms and open-source software for combining computational approximations with regularization for analyzing large datasets as well as providing theoretical justification for their statistical properties. A more complete picture of the interplay between statistical regularization, computational approximation, and scientific parsimony will enable fundamental scientific advancement. The PI will employ the methodologies developed in this project to facilitate novel science with large datasets in climate science, biology, music analysis, astronomy, economics, and chemistry. Furthermore, the PI will carefully integrate the research aims with educational and outreach objectives to engage elementary and high school music students, introducing them to modern statistics and computer science, as well as integrating underrepresented populations in research.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Computational tractability and statistical efficiency for large datasets necessitate approximation or regularization, either of which heuristically balances fidelity to the data with scientific goals like parsimony, smoothness, sparsity, or interpretability. The PI seeks to elucidate the dual roles of regularization and approximation as tools for better scientific understanding. Current research in computer science has focused on improving algorithms so as to enable computation with minimal approximation. Meanwhile, statisticians have developed regularization techniques in order to take advantage of simple structures - graph topologies, sparse linear models, smooth functions - that, if representative of the truth, will improve inference and prediction. The challenge is to understand the consequences of this coupling for scientific interpretability. The PI seeks to address two important issues (1) how do we select tuning parameters when computations are at a premium? and (2) how does the accuracy and stability of scientific conclusions relate to the approximation/regularization methods used to obtain those conclusions? This project seeks to enable fundamental scientific progress by understanding the connections between computational approximations and statistical regularization, thereby facilitating improved inferences. The PI will fill this gap by deriving practical algorithms with accompanying theoretical justification under more reasonable statistical assumptions. These tools will be tightly coupled with applications in neuroscience, genetics, atmospheric science, and music.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>06/07/2018</MinAmdLetterDate>
<MaxAmdLetterDate>06/07/2018</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>1753171</AwardID>
<Investigator>
<FirstName>Daniel</FirstName>
<LastName>McDonald</LastName>
<EmailAddress>dajmcdon@indiana.edu</EmailAddress>
<StartDate>06/07/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Indiana University</Name>
<CityName>Bloomington</CityName>
<ZipCode>474013654</ZipCode>
<PhoneNumber>8128550516</PhoneNumber>
<StreetAddress>509 E 3RD ST</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
</Institution>
<ProgramElement>
<Code>1269</Code>
<Text>STATISTICS</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER: FACULTY EARLY CAR DEV</Text>
</ProgramReference>
</Award>
</rootTag>
