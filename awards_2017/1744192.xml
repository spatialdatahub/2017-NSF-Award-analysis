<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>I-Corps: Dexterous Robotic Prosthetic Control Using Deep Learning Pattern Prediction from Ultrasound Signal</AwardTitle>
<AwardEffectiveDate>07/01/2017</AwardEffectiveDate>
<AwardExpirationDate>07/31/2018</AwardExpirationDate>
<AwardAmount>50000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Anita J. LaSalle</SignBlockName>
</ProgramOfficer>
<AbstractNarration>The broader impact/commercial potential of this I-Corps project lies in the development a novel system that would allow people with transradial and partial hand amputations to gain unparalleled precise individuation of prosthetic digit motion including continuous and simultaneous movement for individual digits without requiring long and complicated training process. To allow for such functionality a novel set of deep learning algorithms are designed to model muscle movement patterns from ultrasound images. The network is pre-trained with a large amount of data in an effort to minimize later individual training. In addition to power prosthetics, the proposed technology can provide broad impacts in other markets where easy-to-use and accurate gestural control of robotics and/or digital environment are required. These include tele-robotics, exoskeleton operation, virtual reality, gaming, glove boxes as well as work related Personal Protective Equipment, and Performance Augmentation and Amplification Devices.&lt;br/&gt;&lt;br/&gt;This I-Corps project will develop and utilize a novel ultrasound sensor and novel deep learning algorithms to recognize continuous muscle activity patterns that can predict accurate and dexterous finger motion. Current myoelectric powered prostheses use discrete classifiers that can only predict a limited number of discrete gestures from noisy electromyography (EMG) signal. Feeding deep learning architectures, such as Convolutional Neural Networks, with rich and detailed ultrasound signal promises to allow for the modeling and prediction of detailed continuous and simultaneous muscle movements patterns, which can be mapped to control continuous and simultaneous movements of individual prosthetic fingers. An additional intellectual of this project merit is the pre-training of these deep neural network with a large amount of data, which would allow for short fine tuning training for individual users, allowing for wide and easy adoption of the technology. The proposed project could therefore allow amputees and people with upper body disabilities to perform finger-by-finger movement activities such as fine object manipulation, typing or playing a musical instrument.</AbstractNarration>
<MinAmdLetterDate>07/12/2017</MinAmdLetterDate>
<MaxAmdLetterDate>07/12/2017</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>1744192</AwardID>
<Investigator>
<FirstName>Gil</FirstName>
<LastName>Weinberg</LastName>
<EmailAddress>gil.weinberg@coa.gatech.edu</EmailAddress>
<StartDate>07/12/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgia Tech Research Corporation</Name>
<CityName>Atlanta</CityName>
<ZipCode>303320420</ZipCode>
<PhoneNumber>4048944819</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
</Institution>
<ProgramElement>
<Code>8023</Code>
<Text>I-Corps</Text>
</ProgramElement>
</Award>
</rootTag>
