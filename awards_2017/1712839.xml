<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research:   Higher-Order Asymptotics and Accurate Inference for Post-Selection</AwardTitle>
<AwardEffectiveDate>08/01/2017</AwardEffectiveDate>
<AwardExpirationDate>07/31/2020</AwardExpirationDate>
<AwardAmount>119995</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Gabor J. Szekely</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Many statistical analyses utilize a model selection procedure. Perhaps the most common model selection problem is that of variable selection in linear regression. Principled motivations for selection include the desire for interpretability, prevention of over-fitting, and concerns about statistical power. A practical motivation arises when the data are high-dimensional, with more explanatory variables than observations. Relevant applications span the entire domain of scientific inquiry, from neuroscience, medicine and physics, to economics, sociology, and psychology. A large catalogue of variable selection procedures is now available, and the statistics community has turned its focus to the question of inference after selection. Standard methods of statistical inference are no longer valid when the same data are used to both select a model and make inferences about that model. It is fundamentally important to have accurate post-selection inference procedures that are also powerful enough to detect observed departures from scientific hypotheses and that avoid the strong distributional assumptions needed for exact inference in finite samples. This research aims to develop post-selection inference methodology that is both accurate and powerful, with particular emphasis on reducing statistical errors that depend on the sample size. &lt;br/&gt;&lt;br/&gt;The goal of this project is to further understanding of the asymptotic theory of post-selection inference, particularly selective inference based on the CovTest and truncated Gaussian (TG) statistic, as well as simultaneous inference using the post-selection intervals (PoSI) procedure. The first two procedures can be motivated by selective error control, i.e., error control for the selected model parameters. The PoSI method seeks to control family-wise error rates for all possible sub-model parameters. While these procedures yield valid post-selection inference, without strong assumptions they are particularly vulnerable to the effects of violation of key assumptions in the realistic setting of small to moderate sample sizes, such as overly-conservative or inaccurate confidence intervals, and low power. In this project, asymptotic expansions, saddle-point approximations, the bootstrap, and related techniques from higher-order asymptotics will be employed to improve accuracy and power for these post-selection inference procedures.</AbstractNarration>
<MinAmdLetterDate>08/02/2017</MinAmdLetterDate>
<MaxAmdLetterDate>08/02/2017</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>1712839</AwardID>
<Investigator>
<FirstName>John</FirstName>
<LastName>Kolassa</LastName>
<EmailAddress>kolassa@stat.rutgers.edu</EmailAddress>
<StartDate>08/02/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Rutgers University New Brunswick</Name>
<CityName>Piscataway</CityName>
<ZipCode>088543925</ZipCode>
<PhoneNumber>8489320150</PhoneNumber>
<StreetAddress>33 Knightsbridge Road</StreetAddress>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
</Institution>
<ProgramElement>
<Code>1269</Code>
<Text>STATISTICS</Text>
</ProgramElement>
</Award>
</rootTag>
