<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF:Small: Optimization of Parallel and Shared Cache Memory using the Footprint Theory</AwardTitle>
<AwardEffectiveDate>08/15/2017</AwardEffectiveDate>
<AwardExpirationDate>07/31/2020</AwardExpirationDate>
<AwardAmount>472262</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Y. Chtchelkanova</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Multicore processors bring a tremendous increase in computing power to personal, scientific, and business computing platforms. Most on-chip memory in these processors is devoted to shared cache, making it a major factor in performance.  A common practice is to improve cache performance by building a path of improvements through testing.  However, testing may take too many steps, it may not converge to a stable solution, and most importantly for large scale parallel programs, the solution is far from optimal since testing covers only a minuscule fraction of all possibilities.  &lt;br/&gt;Instead of testing, this research provides software developers and hardware engineers new tools based on modeling.  The research builds on the past NSF supported research which has developed the footprint theory for sequential applications. This work solves the new problems of data sharing and extends the locality theory to optimize the parallel code. Parallel systems require complex models, and this complexity is addressed by composable models to solve large scale problems with large scale modeling. The new tools include statistical models of both program and machine characteristics.  Program models include profiling analysis that derive the data and cache sharing in all thread combinations and data placements, in cache of all sizes. Cache models analyze the effect of many hardware designs including associativity, exclusivity, coherence, cache-way partitioning, and transient loads/stores. Combined use of these models enables parallel program optimization and improves thread and data placement.</AbstractNarration>
<MinAmdLetterDate>08/07/2017</MinAmdLetterDate>
<MaxAmdLetterDate>04/20/2018</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>1717877</AwardID>
<Investigator>
<FirstName>Chen</FirstName>
<LastName>Ding</LastName>
<EmailAddress>cding@cs.rochester.edu</EmailAddress>
<StartDate>08/07/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Rochester</Name>
<CityName>Rochester</CityName>
<ZipCode>146270140</ZipCode>
<PhoneNumber>5852754031</PhoneNumber>
<StreetAddress>518 HYLAN, RC BOX 270140</StreetAddress>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
</Institution>
<ProgramElement>
<Code>7798</Code>
<Text>SOFTWARE &amp; HARDWARE FOUNDATION</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7942</Code>
<Text>HIGH-PERFORMANCE COMPUTING</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>RES EXPER FOR UNDERGRAD-SUPPLT</Text>
</ProgramReference>
</Award>
</rootTag>
