<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Computational Methods for Hierarchical Manifold Learning</AwardTitle>
<AwardEffectiveDate>08/01/2017</AwardEffectiveDate>
<AwardExpirationDate>07/31/2020</AwardExpirationDate>
<AwardAmount>329954</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Yong Zeng</SignBlockName>
</ProgramOfficer>
<AbstractNarration>The enormous practical success of deep learning warrants a clear and concise mathematical explanation. Although the components of a neural network, and the rules for propagation of information through the layers, are extremely simple, there is to date a lack of a corresponding deep understanding of the roles of the various mechanisms involved. Second, there is a lack of transparency: While a given set of network weights may fit scientific or engineering data in-sample and even generalize well out-of-sample, using a neural net to explain the data or the system producing the data is usually very difficult or impossible.  In this project, the PI will leverage recent progress in mathematical methods from applied and computational harmonic analysis to develop a hierarchical algorithm, based on manifold learning, to replicate the strikingly successful properties of deep learning while adding improved accuracy, adaptability to data, smoothness priors, and transparency.&lt;br/&gt;&lt;br/&gt;A sequence of computational projects is planned to develop a hierarchical algorithm for deep learning, based on representing manifolds by the Laplace-Beltrami operator. Proposed work supports the construction of a complete algorithm that uses layers of manifold learning kernel methods to represent data in a deep manifold learning infrastructure.  The development of the learning algorithm consists of three parts: (1) the construction of a hierarchical manifold learning architecture, using eigenfunctions of the Laplace-Beltrami operator to represent data, and replicating the sharing and pooling features of neural networks, (2) building innovative algorithms for the purpose of optimizing the solution to identification problems, and (3) development of resampling methods to handle large data sets.</AbstractNarration>
<MinAmdLetterDate>06/13/2017</MinAmdLetterDate>
<MaxAmdLetterDate>06/13/2017</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>1723175</AwardID>
<Investigator>
<FirstName>Timothy</FirstName>
<LastName>Sauer</LastName>
<EmailAddress>tsauer@gmu.edu</EmailAddress>
<StartDate>06/13/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>George Mason University</Name>
<CityName>FAIRFAX</CityName>
<ZipCode>220304422</ZipCode>
<PhoneNumber>7039932295</PhoneNumber>
<StreetAddress>4400 UNIVERSITY DR</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<StateCode>VA</StateCode>
</Institution>
<ProgramElement>
<Code>1266</Code>
<Text>APPLIED MATHEMATICS</Text>
</ProgramElement>
<ProgramElement>
<Code>1271</Code>
<Text>COMPUTATIONAL MATHEMATICS</Text>
</ProgramElement>
<ProgramElement>
<Code>8069</Code>
<Text>CDS&amp;E-MSS</Text>
</ProgramElement>
<ProgramReference>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramReference>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
</Award>
</rootTag>
