<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Towards a Formal Theory of Blameworthiness, Intention, and Moral Responsibility</AwardTitle>
<AwardEffectiveDate>08/01/2017</AwardEffectiveDate>
<AwardExpirationDate>07/31/2020</AwardExpirationDate>
<AwardAmount>426998</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>James Donlon</SignBlockName>
</ProgramOfficer>
<AbstractNarration>As we move to an era of self-driving cars, autonomous drones, and robots helping people in work and everyday tasks, there is an increasing need to develop a theory of moral or ethical behavior.  Policy-makers are already grappling with some of the issues.  Jurisdictions around the world are beginning to formulate highway codes that take into consideration the presence of driverless cars.  The goal of this project is to provide the foundations for a theory of blameworthiness, intention, and moral responsibility for such agents and applications.  The study will evaluate this theory against the moral judgments that people make, and take the first steps to applying resulting policies in practice.  &lt;br/&gt;  &lt;br/&gt;A good theory will have to take into consideration tradeoffs among many competing and sometimes conflicting goals.  This suggests that probability and utility will be involved.  The theory will also have to deal with counterfactuals--that is, reasoning about what would have happened had circumstances or actions differed.  Issues of causality will reveal whether an agent's action was responsible for an outcome, or if the action was coincidental to the outcome.  Yet another relevant issue is determining intent: that is, determining whether an agent intended an outcome when performing an act. Pearl's structural-equations framework can model counterfactuals (interventions) and has been used as the basis of a definition of causality.  In this project, the framework will be extended to capture degree of blameworthiness and intention.  These definitions are given relative to an agent's epistemic state, which describes the agent's beliefs and preferences.  Initially, the beliefs will be specified by a probability measure and the preferences by a utility function.  The framework will then be extended to consider more qualitative representations of beliefs and preferences, to take into account issues of normality and typicality (e.g., to capture societal conventions), and to settings with multiple agents (e.g., multiple communicating driverless cars or robots), where game-theoretic concerns and issues of group responsibility vs. individual responsibility arise.  The definitions developed under this framework will be tested to see if they capture people's intuitions about these issues, and  will be applied to situations of practical interest.   The theory will then be refined to take into account the outcomes of these experiments.</AbstractNarration>
<MinAmdLetterDate>07/27/2017</MinAmdLetterDate>
<MaxAmdLetterDate>07/27/2017</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>1718108</AwardID>
<Investigator>
<FirstName>Joseph</FirstName>
<LastName>Halpern</LastName>
<EmailAddress>halpern@cs.cornell.edu</EmailAddress>
<StartDate>07/27/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Cornell University</Name>
<CityName>Ithaca</CityName>
<ZipCode>148502820</ZipCode>
<PhoneNumber>6072555014</PhoneNumber>
<StreetAddress>373 Pine Tree Road</StreetAddress>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
</Institution>
<ProgramElement>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
</Award>
</rootTag>
