<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF: Small: A Theory of Explanation Languages</AwardTitle>
<AwardEffectiveDate>09/01/2017</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardAmount>499897</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sol J. Greenspan</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Trust in a computing system depends on understanding its behavior. When users are confronted with unexpected and unexplained results, they may not be willing to rely on such a system anymore. With computing systems controlling ever-increasing parts of our lives, it is important to ensure that they are accountable and can justify their decisions and actions. For example, we want to know how a decision support system arrived at its suggestion for a particular health-care plan or why a self-driving car could not avoid an accident. Despite this need for understanding computing systems, few systems today explain their behavior to their users. This research explores the nature of explanations and how they can be employed in the creation of computing systems that can explain their behavior to programmers and users.&lt;br/&gt;&lt;br/&gt; Based on an analysis of the nature of explanations and their properties, this research explores operations for the composition and transformation of explanations and the alignment with computations. Part of the research is the development of criteria to judge the quality of explanations and develop guidelines for the design and implementation of explanation languages. Altogether, this research produces a theory of explanation languages. The development of example explanation languages as suggested by the developed theory and their evaluation according to the explanation quality criteria supports the assessment of the research progress. The broader impact of this project includes support for graduate, undergraduate, and high school students who will carry the ideas developed in this project out into the world. Moreover, a theory of explanations and a framework for systematically adding explanations to computing systems empowers software developers to make their products more widely understandable and trustworthy, which ultimately enhances the acceptance of new technologies in society. This research leads to methods and tools that affect the design of future software. Given the importance of software in all aspects of our lives and the growing need for understanding its behaviors, the middle- and long-term practical impact of this research can be enormous.</AbstractNarration>
<MinAmdLetterDate>05/25/2017</MinAmdLetterDate>
<MaxAmdLetterDate>05/25/2017</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>1717300</AwardID>
<Investigator>
<FirstName>Martin</FirstName>
<LastName>Erwig</LastName>
<EmailAddress>erwig@oregonstate.edu</EmailAddress>
<StartDate>05/25/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Oregon State University</Name>
<CityName>Corvallis</CityName>
<ZipCode>973318507</ZipCode>
<PhoneNumber>5417374933</PhoneNumber>
<StreetAddress>OREGON STATE UNIVERSITY</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Oregon</StateName>
<StateCode>OR</StateCode>
</Institution>
<ProgramElement>
<Code>7798</Code>
<Text>SOFTWARE &amp; HARDWARE FOUNDATION</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7944</Code>
<Text>SOFTWARE ENG &amp; FORMAL METHODS</Text>
</ProgramReference>
</Award>
</rootTag>
