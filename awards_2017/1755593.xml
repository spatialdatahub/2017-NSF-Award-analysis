<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII: CHS: Predicting When, Why, and How Multiple People Will Disagree when Answering a Visual Question</AwardTitle>
<AwardEffectiveDate>05/01/2018</AwardEffectiveDate>
<AwardExpirationDate>04/30/2020</AwardExpirationDate>
<AwardAmount>174947</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim P. Glinert</SignBlockName>
</ProgramOfficer>
<AbstractNarration>The goal of a visual question answering (VQA) system is to empower people to find the answer to any question about any image.  For example, a VQA system could enable blind people to address daily visual challenges such as learning whether a pair of socks match or learning what type of food is in a can.  VQA services could also facilitate the creation of smarter environments, say to monitor how many defective products are on a factory assembly line at any given time.  A limitation of existing VQA systems is that they do not account for the fact that a visual question may elicit different answers from different people.  VQA systems could save time and reduce user frustration if they empowered users to anticipate and resolve any answer disagreements that may arise.  Blind and sighted people could more rapidly and accurately learn about the diversity of human perspectives on the visual world.  VQA services also could teach people how to ask visual questions that elicit the desired answer diversity.&lt;br/&gt;&lt;br/&gt;This project will create artificial intelligence (AI) models that can account for the possible diversity of answers inherent in crowd intelligence.  Specifically, AI models will be designed to predict when, why, and how human answer disagreement occurs, which in turn will enable new designs for human-computer partnerships.  This is challenging because it necessitates designing frameworks that simultaneously model and synthesize different and potentially conflicting perceptions of images and language for the many possible causes of disagreement.  To ensure that the AI models generalize across a broad range of applications, an existing corpus of over one million visual questions asked by blind and sighted people will be used to create annotated datasets that indicate when, why, and how much answer disagreement arises.  Methods will then be developed for automatically predicting directly from a visual question how much answer diversity will arise from a crowd, and why disagreement arises when it does.  Finally, a system will be designed for guiding visually-impaired users to more quickly formulate visual questions so they can receive a single, unambiguous crowd response (e.g., guide the person to better frame the visual content of interest with a mobile phone camera).  User studies with blind users will be conducted to empirically test the efficacy of the new system, with a focus on uncovering human-based issues in real-world, real-time situations.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>03/16/2018</MinAmdLetterDate>
<MaxAmdLetterDate>03/16/2018</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>1755593</AwardID>
<Investigator>
<FirstName>Danna</FirstName>
<LastName>Gurari</LastName>
<EmailAddress>danna.gurari@ischool.utexas.edu</EmailAddress>
<StartDate>03/16/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Austin</Name>
<CityName>Austin</CityName>
<ZipCode>787595316</ZipCode>
<PhoneNumber>5124716424</PhoneNumber>
<StreetAddress>3925 W Braker Lane, Ste 3.340</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
</Institution>
<ProgramElement>
<Code>7367</Code>
<Text>Cyber-Human Systems (CHS)</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
</Award>
</rootTag>
