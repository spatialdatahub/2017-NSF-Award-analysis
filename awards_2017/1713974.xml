<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAPSI: Using Prosody to Learn Phonetic Categories</AwardTitle>
<AwardEffectiveDate>06/01/2017</AwardEffectiveDate>
<AwardExpirationDate>05/31/2018</AwardExpirationDate>
<AwardAmount>5400</AwardAmount>
<AwardInstrument>
<Value>Fellowship</Value>
</AwardInstrument>
<Organization>
<Code>01090000</Code>
<Directorate>
<LongName>Office Of The Director</LongName>
</Directorate>
<Division>
<LongName>Office Of Internatl Science &amp;Engineering</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Anne L. Emig</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Speech is highly variable. Though we often do not notice, every time we say a word or sound, it comes out differently. Our best language technologies (e.g. Siri) often fail precisely because of this variability, which shows just how difficult it is to learn about the sounds of language from variable speech. Nonetheless, infants learn about the sounds of their language within the first year of their life, and they do this with much less data and fewer resources than language technologies have access to. This project tests hypotheses about how infants solve this learning problem so effectively, by using computational models to simulate the learning process. This work will be done in collaboration with Dr. Reiko Mazuka at RIKEN Brain Science Institute, who has the only existing dataset with the necessary annotations for this project. Understanding how infants, the most successful language learners, learn can help improve language technologies so that they work more successfully across a wider range of languages and are more usable by a larger proportion of society, including people with accented speech and children, whose speech continues to pose problems for such systems. &lt;br/&gt;&lt;br/&gt;Various unsupervised sound category learning models will be tested on their ability to learn the Japanese vowel contrast, which no previous models have successfully learned. One of the factors that contributes to the difficulty of this particular learning problem is prosody - or the rhythm and intonation of speech - which systematically lengthens vowels in particular positions and causes overlap between short and long vowels through a process known as phrase-final lengthening. Knowing how prosody affects the duration of vowels and factoring out its influence could be helpful in learning the Japanese sound categories. This project will incorporate prosody into two existing computational models of sound category learning and see whether this improves learning outcomes. &lt;br/&gt;&lt;br/&gt;This award, under the East Asia and Pacific Summer Institutes program, supports summer research by a U.S. graduate student and is jointly funded by NSF and the Japan Society for the Promotion of Science.</AbstractNarration>
<MinAmdLetterDate>05/08/2017</MinAmdLetterDate>
<MaxAmdLetterDate>05/08/2017</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>1713974</AwardID>
<Investigator>
<FirstName>Katarzyna</FirstName>
<LastName>Hitczenko</LastName>
<EmailAddress/>
<StartDate>05/08/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Hitczenko               Katarzyna      A</Name>
<CityName>Riverdale</CityName>
<ZipCode>207371008</ZipCode>
<PhoneNumber/>
<StreetAddress/>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
</Institution>
<ProgramElement>
<Code>7316</Code>
<Text>EAPSI</Text>
</ProgramElement>
<ProgramReference>
<Code>5921</Code>
<Text>JAPAN</Text>
</ProgramReference>
<ProgramReference>
<Code>5978</Code>
<Text>EAST ASIA AND PACIFIC PROGRAM</Text>
</ProgramReference>
<ProgramReference>
<Code>7316</Code>
<Text>EAPSI</Text>
</ProgramReference>
</Award>
</rootTag>
